{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNO0OvbjYjNqSE/i6ws4ZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhargaveAC/All4SamplesIntegrated/blob/master/PCN520.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGZduk0-bBP"
      },
      "source": [
        "<center>    \n",
        "PCN520 Research Project\n",
        "<hr>\n",
        "<h1> Utilising Deep Learning for Individualised Quality Assurance in Radiotherapy: DOSE PREDICTION AND OPTIMISATION </h1>\n",
        "<hr>\n",
        "<h3> 3D U-Net Voxel-Wise Dose Prediction Model </h3>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCGie7cqtlA_",
        "outputId": "4c00ccb6-dd35-4231-e798-5b804e77d8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenKBP'...\n",
            "remote: Enumerating objects: 4260, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 4260 (delta 38), reused 30 (delta 29), pack-reused 4208 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4260/4260), 523.57 MiB | 12.35 MiB/s, done.\n",
            "Resolving deltas: 100% (1603/1603), done.\n",
            "Updating files: 100% (4016/4016), done.\n"
          ]
        }
      ],
      "source": [
        "# Get the repo\n",
        "repo_dir = 'OpenKBP'\n",
        "!git clone https://github.com/ababier/open-kbp.git {repo_dir}\n",
        "\n",
        "# Add repo to path\n",
        "import sys\n",
        "sys.path.append(repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "uCeav4kOudKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preamble**: Import all the libraries here."
      ],
      "metadata": {
        "id": "6eezUgkn9nUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Optional, Union, List, Dict, Iterator\n",
        "from collections import OrderedDict\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.typing import NDArray\n",
        "from more_itertools import windowed\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XysOqcF09kgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we run anything, first define the paths where the provided data is stored and where the results (e.g., models, predictions) should be saved."
      ],
      "metadata": {
        "id": "RMu1JCNU36I4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJEIaar-d2r",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Define project directories\n",
        "primary_directory = Path(repo_dir).resolve()  # directory where everything is stored\n",
        "provided_data_dir = primary_directory / \"provided-data\"\n",
        "training_data_dir = provided_data_dir / \"train-pats\"\n",
        "validation_data_dir = provided_data_dir / \"validation-pats\"\n",
        "testing_data_dir = provided_data_dir / \"test-pats\"\n",
        "results_dir = primary_directory / \"results\"  # where any data generated by this code (e.g., predictions, models) are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes for data loading"
      ],
      "metadata": {
        "id": "chP-o4Lp974o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataBatch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dose: Optional[torch.Tensor] = None,\n",
        "        predicted_dose: Optional[torch.Tensor] = None,\n",
        "        ct: Optional[torch.Tensor] = None,\n",
        "        structure_masks: Optional[torch.Tensor] = None,\n",
        "        structure_mask_names: Optional[List[str]] = None,\n",
        "        possible_dose_mask: Optional[torch.Tensor] = None,\n",
        "        voxel_dimensions: Optional[torch.Tensor] = None,\n",
        "        patient_list: Optional[List[str]] = None,\n",
        "        patient_path_list: Optional[List[Path]] = None,\n",
        "    ):\n",
        "        self.dose = dose\n",
        "        self.predicted_dose = predicted_dose\n",
        "        self.ct = ct\n",
        "        self.structure_masks = structure_masks\n",
        "        self.structure_mask_names = structure_mask_names\n",
        "        self.possible_dose_mask = possible_dose_mask\n",
        "        self.voxel_dimensions = voxel_dimensions\n",
        "        self.patient_list = patient_list\n",
        "        self.patient_path = patient_path_list\n",
        "\n",
        "    @classmethod\n",
        "    def initialize_from_required_data(cls, data_dimensions: dict[str, torch.Tensor], batch_size: int) -> DataBatch:\n",
        "        attribute_values = {}\n",
        "        for data, dimensions in data_dimensions.items():\n",
        "            batch_data_dimensions = (batch_size, *dimensions)\n",
        "            attribute_values[data] = torch.zeros(batch_data_dimensions, dtype=torch.float32)\n",
        "        return cls(**attribute_values)\n",
        "\n",
        "    def set_values(self, data_name: str, batch_index: int, values: torch.Tensor):\n",
        "        getattr(self, data_name)[batch_index] = values\n",
        "\n",
        "    def get_index_structure_from_structure(self, structure_name: str):\n",
        "        return self.structure_mask_names.index(structure_name)\n",
        "\n",
        "\n",
        "class DataShapes:\n",
        "    def __init__(self, num_rois):\n",
        "        self.num_rois = num_rois\n",
        "        self.patient_shape = (128, 128, 128)\n",
        "\n",
        "    @property\n",
        "    def dose(self) -> torch.Size:\n",
        "        return torch.Size(self.patient_shape + (1,))\n",
        "\n",
        "    @property\n",
        "    def predicted_dose(self) -> torch.Size:\n",
        "        return self.dose\n",
        "\n",
        "    @property\n",
        "    def ct(self) -> torch.Size:\n",
        "        return torch.Size(self.patient_shape + (1,))\n",
        "\n",
        "    @property\n",
        "    def structure_masks(self) -> torch.Size:\n",
        "        return torch.Size(self.patient_shape + (self.num_rois,))\n",
        "\n",
        "    @property\n",
        "    def possible_dose_mask(self) -> torch.Size:\n",
        "        return torch.Size(self.patient_shape + (1,))\n",
        "\n",
        "    @property\n",
        "    def voxel_dimensions(self) -> torch.Size:\n",
        "        return torch.Size([3])\n",
        "\n",
        "    def from_data_names(self, data_names: list[str]) -> dict[str, torch.Size]:\n",
        "        data_shapes = {}\n",
        "        for name in data_names:\n",
        "            data_shapes[name] = getattr(self, name)\n",
        "        return data_shapes\n",
        "\n",
        "\n",
        "def load_file(file_path: Path) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "    if file_path.stem == \"voxel_dimensions\":\n",
        "        return torch.tensor(np.loadtxt(file_path), dtype=torch.float32)\n",
        "\n",
        "    loaded_file_df = pd.read_csv(file_path, index_col=0)\n",
        "    if loaded_file_df.shape[1] == 1:  # Check if the loaded file has only one column\n",
        "        loaded_file = {\n",
        "            \"indices\": torch.tensor(loaded_file_df.index.values, dtype=torch.int64),\n",
        "            \"data\": torch.tensor(loaded_file_df.iloc[:, 0].values, dtype=torch.float32)  # Extract data from the single column\n",
        "        }\n",
        "    elif loaded_file_df.isnull().values.any():\n",
        "        loaded_file = torch.tensor(np.array(loaded_file_df.index).squeeze(), dtype=torch.float32)\n",
        "    else:\n",
        "        loaded_file = {\n",
        "            \"indices\": torch.tensor(loaded_file_df.index.values, dtype=torch.int64),\n",
        "            \"data\": torch.tensor(loaded_file_df.data.values, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "    return loaded_file\n",
        "\n",
        "\n",
        "class DataLoaderCustom:\n",
        "    def __init__(self, patient_paths: List[Path], batch_size: int = 2):\n",
        "        self.patient_paths = patient_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.paths_by_patient_id = {patient_path.stem: patient_path for patient_path in self.patient_paths}\n",
        "        self.required_files: Optional[Dict] = None\n",
        "        self.mode_name: Optional[str] = None\n",
        "        self.rois = dict(\n",
        "            oars=[\"Brainstem\", \"SpinalCord\", \"RightParotid\", \"LeftParotid\", \"Esophagus\", \"Larynx\", \"Mandible\"],\n",
        "            targets=[\"PTV56\", \"PTV63\", \"PTV70\"],\n",
        "        )\n",
        "        self.full_roi_list = sum(map(list, self.rois.values()), [])\n",
        "        self.num_rois = len(self.full_roi_list)\n",
        "        self.data_shapes = DataShapes(self.num_rois)\n",
        "\n",
        "    @property\n",
        "    def patient_id_list(self) -> List[str]:\n",
        "        return list(self.paths_by_patient_id.keys())\n",
        "\n",
        "    def get_batches(self) -> Iterator[DataBatch]:\n",
        "        batches = windowed(self.patient_paths, n=self.batch_size, step=self.batch_size)\n",
        "        complete_batches = (batch for batch in batches if None not in batch)\n",
        "        for batch_paths in tqdm(complete_batches):\n",
        "            yield self.prepare_data(batch_paths)\n",
        "\n",
        "    def get_patients(self, patient_list: List[str]) -> DataBatch:\n",
        "        file_paths_to_load = [self.paths_by_patient_id[patient] for patient in patient_list]\n",
        "        return self.prepare_data(file_paths_to_load)\n",
        "\n",
        "    def set_mode(self, mode: str) -> None:\n",
        "        self.mode_name = mode\n",
        "        if mode == \"training_model\":\n",
        "            required_data = [\"dose\", \"ct\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n",
        "        elif mode == \"predicted_dose\":\n",
        "            required_data = [mode]\n",
        "            self._force_batch_size_one()\n",
        "        elif mode == \"evaluation\":\n",
        "            required_data = [\"dose\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n",
        "            self._force_batch_size_one()\n",
        "        elif mode == \"dose_prediction\":\n",
        "            required_data = [\"ct\", \"structure_masks\", \"possible_dose_mask\", \"voxel_dimensions\"]\n",
        "            self._force_batch_size_one()\n",
        "        else:\n",
        "            raise ValueError(f\"Mode `{mode}` does not exist. Mode must be either training_model, prediction, predicted_dose, or evaluation\")\n",
        "        self.required_files = self.data_shapes.from_data_names(required_data)\n",
        "\n",
        "    def _force_batch_size_one(self) -> None:\n",
        "        if self.batch_size != 1:\n",
        "            self.batch_size = 1\n",
        "            Warning(\"Batch size has been changed to 1 for dose prediction mode\")\n",
        "\n",
        "    def shuffle_data(self) -> None:\n",
        "        np.random.shuffle(self.patient_paths)\n",
        "\n",
        "    def prepare_data(self, file_paths_to_load: List[Path]) -> DataBatch:\n",
        "        batch_data = DataBatch.initialize_from_required_data(self.required_files, self.batch_size)\n",
        "        for batch_idx, path in enumerate(file_paths_to_load):\n",
        "            for data_name, data in self.required_files.items():\n",
        "                if data_name == \"structure_masks\":\n",
        "                    for roi_idx, roi in enumerate(self.full_roi_list):\n",
        "                        roi_file_path = Path(path / \"{}.csv\".format(roi))\n",
        "                        if not roi_file_path.exists():\n",
        "                            continue\n",
        "                        file_data = load_file(roi_file_path)\n",
        "                        file_data_dense = self.dense_scatter(file_data)\n",
        "                        file_data_dense = file_data_dense.unsqueeze(-1)  # Ensure shape (128, 128, 128, 1)\n",
        "                        batch_data.structure_masks[..., roi_idx] = file_data_dense.squeeze()\n",
        "                    batch_data.structure_mask_names = self.full_roi_list\n",
        "                else:\n",
        "                    file_path = Path(path / \"{}.csv\".format(data_name))\n",
        "                    if not file_path.exists():\n",
        "                        continue\n",
        "                    file_data = load_file(file_path)\n",
        "                    file_data_dense = self.dense_scatter(file_data)\n",
        "                    file_data_dense = file_data_dense.unsqueeze(-1)  # Ensure shape (128, 128, 128, 1)\n",
        "                    batch_data.set_values(data_name, batch_idx, file_data_dense)\n",
        "            batch_data.patient_list = [os.path.basename(path) for path in file_paths_to_load]\n",
        "            batch_data.patient_path_list = [path for path in file_paths_to_load]\n",
        "        return batch_data\n",
        "\n",
        "\n",
        "    def dense_scatter(self, sparse_vector: dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        indices = sparse_vector[\"indices\"].long()\n",
        "        print(f\"indices shape: {indices.shape}\")  # Debugging statement\n",
        "        print(f\"indices: {indices}\")  # Debugging statement\n",
        "        flat_size = torch.prod(torch.tensor(self.data_shapes.patient_shape))\n",
        "        if torch.any(indices >= flat_size) or torch.any(indices < 0):\n",
        "           raise IndexError(\"Indices are out of bounds for the dense array shape.\")\n",
        "        dense_data = torch.zeros(self.data_shapes.patient_shape, dtype=torch.float32)\n",
        "        dense_data = dense_data.view(-1)\n",
        "        dense_data[indices] = sparse_vector[\"data\"]\n",
        "        return dense_data.view(self.data_shapes.patient_shape)\n",
        "\n",
        "    def sparse_scatter(self, data: torch.Tensor, sparse: bool = True) -> dict[str, torch.Tensor]:\n",
        "        if sparse:\n",
        "            data = sparse_vector_function(data)\n",
        "        else:\n",
        "            data = data.view(-1)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "ny89N5aw-Be9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "def load_data(data_dir: str):\n",
        "    data_dir = Path(data_dir)\n",
        "    patient_dirs = [x for x in data_dir.iterdir() if x.is_dir()]\n",
        "\n",
        "    data_loader = DataLoaderCustom(patient_dirs)\n",
        "    data_loader.set_mode(\"training_model\")\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "f2cwSnu__23E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3D U-Net Model (Voxel-Wise Dose Prediction)"
      ],
      "metadata": {
        "id": "dU9HxmJ-_PR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, init_features=32):\n",
        "        super(UNet3D, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet3D._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet3D._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet3D._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet3D._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet3D._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose3d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet3D._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose3d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet3D._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose3d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet3D._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose3d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet3D._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv3d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv3d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm3d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv3d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm3d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "class DoseLoss(nn.Module):\n",
        "    def __init__(self, max_dose):\n",
        "        super(DoseLoss, self).__init__()\n",
        "        self.max_dose = max_dose\n",
        "\n",
        "    def forward(self, predicted_dose, true_dose, possible_dose_mask):\n",
        "        # Apply possible dose mask\n",
        "        predicted_dose = predicted_dose * possible_dose_mask\n",
        "        true_dose = true_dose * possible_dose_mask\n",
        "\n",
        "        # Compute mean squared error loss\n",
        "        mse_loss = F.mse_loss(predicted_dose, true_dose)\n",
        "\n",
        "        # Normalize by max dose to ensure that the dose values are in the same scale\n",
        "        normalized_loss = mse_loss / self.max_dose\n",
        "\n",
        "        return normalized_loss\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model, data_loader, num_epochs: int, learning_rate: float, max_dose: float\n",
        "):\n",
        "    criterion = DoseLoss(max_dose)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data_batch in enumerate(data_loader.get_batches()):\n",
        "            ct_scans = torch.tensor(data_batch.ct, dtype=torch.float32)\n",
        "            structure_masks = torch.tensor(data_batch.structure_masks, dtype=torch.float32)\n",
        "            possible_dose_masks = torch.tensor(data_batch.possible_dose_mask, dtype=torch.float32)\n",
        "            true_doses = torch.tensor(data_batch.dose, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predicted_doses = model(ct_scans)\n",
        "\n",
        "            loss = criterion(predicted_doses, true_doses, possible_dose_masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:\n",
        "                print(\n",
        "                    f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\"\n",
        "                )\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/OpenKBP/provided-data/train-pats\"  # Replace with your data directory path\n",
        "    data_loader = load_data(data_dir)\n",
        "\n",
        "    # Initialize model\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    model = UNet3D(in_channels, out_channels)\n",
        "\n",
        "    # Training parameters\n",
        "    num_epochs = 20\n",
        "    learning_rate = 0.001\n",
        "    max_dose = 100.0  # Replace with the appropriate max dose for your dataset\n",
        "\n",
        "    # Train model\n",
        "    train_model(model, data_loader, num_epochs, learning_rate, max_dose)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q6_bWP5K_nwl",
        "outputId": "fc70ba7f-4d75-4800-b02d-44cff84a2862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indices shape: torch.Size([69403])\n",
            "indices: tensor([ 581179,  581307,  581435,  ..., 1499689, 1499816, 1499944])\n",
            "indices shape: torch.Size([86095])\n",
            "indices: tensor([ 532027,  532155,  532283,  ..., 1532456, 1532457, 1532458])\n",
            "indices shape: torch.Size([683])\n",
            "indices: tensor([1040157, 1040285, 1040413, 1040541, 1040669, 1056413, 1056415, 1056416,\n",
            "        1056540, 1056541, 1056542, 1056543, 1056544, 1056545, 1056546, 1056668,\n",
            "        1056669, 1056670, 1056671, 1056672, 1056673, 1056674, 1056675, 1056796,\n",
            "        1056797, 1056798, 1056799, 1056800, 1056801, 1056802, 1056803, 1056924,\n",
            "        1056925, 1056926, 1056927, 1056928, 1056929, 1056930, 1056931, 1057052,\n",
            "        1057053, 1057054, 1057055, 1057056, 1057057, 1057058, 1057059, 1057181,\n",
            "        1057182, 1057183, 1057184, 1057185, 1057310, 1057311, 1072671, 1072674,\n",
            "        1072795, 1072797, 1072798, 1072799, 1072800, 1072801, 1072802, 1072803,\n",
            "        1072923, 1072924, 1072925, 1072926, 1072927, 1072928, 1072929, 1072930,\n",
            "        1072931, 1072932, 1072933, 1073051, 1073052, 1073053, 1073054, 1073055,\n",
            "        1073056, 1073057, 1073058, 1073059, 1073060, 1073061, 1073062, 1073179,\n",
            "        1073180, 1073181, 1073182, 1073183, 1073184, 1073185, 1073186, 1073187,\n",
            "        1073188, 1073189, 1073190, 1073194, 1073307, 1073308, 1073309, 1073310,\n",
            "        1073311, 1073312, 1073313, 1073314, 1073315, 1073316, 1073317, 1073318,\n",
            "        1073322, 1073435, 1073436, 1073437, 1073438, 1073439, 1073440, 1073441,\n",
            "        1073442, 1073443, 1073444, 1073445, 1073563, 1073564, 1073565, 1073566,\n",
            "        1073567, 1073568, 1073569, 1073570, 1073571, 1073692, 1073694, 1073695,\n",
            "        1073696, 1073697, 1073698, 1073699, 1089055, 1089058, 1089059, 1089179,\n",
            "        1089180, 1089181, 1089182, 1089183, 1089184, 1089185, 1089186, 1089187,\n",
            "        1089188, 1089189, 1089307, 1089308, 1089309, 1089310, 1089311, 1089312,\n",
            "        1089313, 1089314, 1089315, 1089316, 1089317, 1089318, 1089435, 1089436,\n",
            "        1089437, 1089438, 1089439, 1089440, 1089441, 1089442, 1089443, 1089444,\n",
            "        1089445, 1089446, 1089447, 1089448, 1089449, 1089450, 1089451, 1089452,\n",
            "        1089453, 1089454, 1089563, 1089564, 1089565, 1089566, 1089567, 1089568,\n",
            "        1089569, 1089570, 1089571, 1089572, 1089573, 1089574, 1089575, 1089576,\n",
            "        1089577, 1089578, 1089579, 1089580, 1089581, 1089582, 1089583, 1089584,\n",
            "        1089691, 1089692, 1089693, 1089694, 1089695, 1089696, 1089697, 1089698,\n",
            "        1089699, 1089700, 1089701, 1089702, 1089703, 1089704, 1089705, 1089706,\n",
            "        1089707, 1089708, 1089709, 1089710, 1089711, 1089712, 1089819, 1089820,\n",
            "        1089821, 1089822, 1089823, 1089824, 1089825, 1089826, 1089827, 1089828,\n",
            "        1089829, 1089830, 1089831, 1089833, 1089834, 1089835, 1089836, 1089837,\n",
            "        1089947, 1089948, 1089949, 1089950, 1089951, 1089952, 1089953, 1089954,\n",
            "        1089955, 1089956, 1089957, 1089958, 1090076, 1090078, 1090079, 1090080,\n",
            "        1090081, 1090082, 1090083, 1090084, 1090085, 1090211, 1105439, 1105442,\n",
            "        1105443, 1105563, 1105564, 1105565, 1105566, 1105567, 1105568, 1105569,\n",
            "        1105570, 1105571, 1105572, 1105573, 1105691, 1105692, 1105693, 1105694,\n",
            "        1105695, 1105696, 1105697, 1105698, 1105699, 1105700, 1105701, 1105702,\n",
            "        1105819, 1105820, 1105821, 1105822, 1105823, 1105824, 1105825, 1105826,\n",
            "        1105827, 1105828, 1105829, 1105830, 1105831, 1105832, 1105833, 1105834,\n",
            "        1105835, 1105836, 1105837, 1105838, 1105839, 1105840, 1105947, 1105948,\n",
            "        1105949, 1105950, 1105951, 1105952, 1105953, 1105954, 1105955, 1105956,\n",
            "        1105957, 1105958, 1105959, 1105960, 1105961, 1105962, 1105963, 1105964,\n",
            "        1105965, 1105966, 1105967, 1105968, 1106075, 1106076, 1106077, 1106078,\n",
            "        1106079, 1106080, 1106081, 1106082, 1106083, 1106084, 1106085, 1106086,\n",
            "        1106087, 1106088, 1106089, 1106090, 1106091, 1106092, 1106093, 1106094,\n",
            "        1106095, 1106096, 1106203, 1106204, 1106205, 1106206, 1106207, 1106208,\n",
            "        1106209, 1106210, 1106211, 1106212, 1106213, 1106214, 1106215, 1106216,\n",
            "        1106217, 1106218, 1106219, 1106220, 1106221, 1106222, 1106223, 1106224,\n",
            "        1106331, 1106332, 1106333, 1106334, 1106335, 1106336, 1106337, 1106338,\n",
            "        1106339, 1106340, 1106341, 1106342, 1106343, 1106460, 1106462, 1106463,\n",
            "        1106464, 1106465, 1106466, 1106467, 1106468, 1106469, 1106595, 1121827,\n",
            "        1121949, 1121950, 1121951, 1121952, 1121953, 1121954, 1121955, 1121956,\n",
            "        1121957, 1122075, 1122076, 1122077, 1122078, 1122079, 1122080, 1122081,\n",
            "        1122082, 1122083, 1122084, 1122085, 1122086, 1122087, 1122088, 1122089,\n",
            "        1122203, 1122204, 1122205, 1122206, 1122207, 1122208, 1122209, 1122210,\n",
            "        1122211, 1122212, 1122213, 1122214, 1122215, 1122216, 1122217, 1122218,\n",
            "        1122219, 1122220, 1122221, 1122222, 1122223, 1122331, 1122332, 1122333,\n",
            "        1122334, 1122335, 1122336, 1122337, 1122338, 1122339, 1122340, 1122341,\n",
            "        1122342, 1122343, 1122344, 1122345, 1122346, 1122347, 1122348, 1122349,\n",
            "        1122350, 1122351, 1122352, 1122459, 1122460, 1122461, 1122462, 1122463,\n",
            "        1122464, 1122465, 1122466, 1122467, 1122468, 1122469, 1122470, 1122471,\n",
            "        1122472, 1122473, 1122474, 1122475, 1122476, 1122477, 1122478, 1122479,\n",
            "        1122480, 1122587, 1122588, 1122589, 1122590, 1122591, 1122592, 1122593,\n",
            "        1122594, 1122595, 1122596, 1122597, 1122598, 1122599, 1122600, 1122601,\n",
            "        1122602, 1122603, 1122604, 1122605, 1122606, 1122607, 1122608, 1122715,\n",
            "        1122716, 1122717, 1122718, 1122719, 1122720, 1122721, 1122722, 1122723,\n",
            "        1122724, 1122725, 1122726, 1122727, 1122728, 1122729, 1122844, 1122846,\n",
            "        1122847, 1122848, 1122849, 1122850, 1122851, 1122852, 1122853, 1122854,\n",
            "        1122979, 1138211, 1138335, 1138339, 1138459, 1138460, 1138461, 1138462,\n",
            "        1138463, 1138464, 1138465, 1138466, 1138467, 1138468, 1138469, 1138470,\n",
            "        1138471, 1138472, 1138587, 1138588, 1138589, 1138590, 1138591, 1138592,\n",
            "        1138593, 1138594, 1138595, 1138596, 1138597, 1138598, 1138599, 1138600,\n",
            "        1138601, 1138602, 1138604, 1138715, 1138716, 1138717, 1138718, 1138719,\n",
            "        1138720, 1138721, 1138722, 1138723, 1138724, 1138725, 1138726, 1138727,\n",
            "        1138728, 1138729, 1138730, 1138731, 1138732, 1138733, 1138734, 1138735,\n",
            "        1138843, 1138844, 1138845, 1138846, 1138847, 1138848, 1138849, 1138850,\n",
            "        1138851, 1138852, 1138853, 1138854, 1138855, 1138856, 1138857, 1138858,\n",
            "        1138859, 1138860, 1138861, 1138862, 1138863, 1138971, 1138972, 1138973,\n",
            "        1138974, 1138975, 1138976, 1138977, 1138978, 1138979, 1138980, 1138981,\n",
            "        1138982, 1138983, 1138984, 1138985, 1138986, 1138987, 1138988, 1139102,\n",
            "        1139103, 1139104, 1139105, 1139106, 1139107, 1139108, 1139109, 1139110,\n",
            "        1139111, 1139112, 1139113, 1139235, 1139237, 1139238, 1154971, 1154972,\n",
            "        1154974, 1154981, 1154982, 1154983, 1155099, 1155100, 1155101, 1155102,\n",
            "        1155103, 1155104, 1155109, 1155110, 1155111, 1155227, 1155228, 1155230,\n",
            "        1155231, 1155232, 1155237, 1155238, 1155355, 1155358, 1155366, 1155367,\n",
            "        1155494, 1171483, 1171611])\n",
            "indices shape: torch.Size([532])\n",
            "indices: tensor([1089465, 1089466, 1089467, 1089585, 1089586, 1089590, 1089592, 1089593,\n",
            "        1089594, 1089595, 1089596, 1089597, 1089598, 1089599, 1089600, 1089601,\n",
            "        1089602, 1089603, 1089604, 1089605, 1089606, 1089607, 1089608, 1089609,\n",
            "        1089610, 1089713, 1089714, 1089718, 1089720, 1089721, 1089722, 1089723,\n",
            "        1089724, 1089725, 1089726, 1089727, 1089728, 1089729, 1089730, 1089731,\n",
            "        1089732, 1089733, 1089734, 1089735, 1089736, 1089737, 1089738, 1089852,\n",
            "        1089853, 1089854, 1089855, 1089856, 1089857, 1089858, 1089859, 1089860,\n",
            "        1105841, 1105842, 1105843, 1105844, 1105845, 1105846, 1105847, 1105848,\n",
            "        1105849, 1105850, 1105851, 1105852, 1105853, 1105854, 1105858, 1105859,\n",
            "        1105866, 1105867, 1105969, 1105970, 1105971, 1105972, 1105973, 1105974,\n",
            "        1105975, 1105976, 1105977, 1105978, 1105979, 1105980, 1105981, 1105982,\n",
            "        1105983, 1105984, 1105985, 1105986, 1105987, 1105988, 1105989, 1105990,\n",
            "        1105991, 1105992, 1105993, 1105994, 1105995, 1105996, 1105997, 1105998,\n",
            "        1105999, 1106000, 1106097, 1106098, 1106099, 1106100, 1106101, 1106102,\n",
            "        1106103, 1106104, 1106105, 1106106, 1106107, 1106108, 1106109, 1106110,\n",
            "        1106111, 1106112, 1106113, 1106114, 1106115, 1106116, 1106117, 1106118,\n",
            "        1106119, 1106120, 1106121, 1106122, 1106123, 1106124, 1106125, 1106126,\n",
            "        1106127, 1106128, 1106225, 1106228, 1106230, 1106232, 1106234, 1106236,\n",
            "        1106237, 1106238, 1106239, 1106240, 1106241, 1106242, 1106243, 1106244,\n",
            "        1106246, 1106252, 1122225, 1122226, 1122227, 1122228, 1122229, 1122231,\n",
            "        1122232, 1122233, 1122234, 1122235, 1122256, 1122257, 1122258, 1122259,\n",
            "        1122260, 1122353, 1122354, 1122355, 1122356, 1122357, 1122358, 1122359,\n",
            "        1122360, 1122361, 1122362, 1122363, 1122364, 1122365, 1122366, 1122367,\n",
            "        1122368, 1122369, 1122370, 1122372, 1122374, 1122375, 1122376, 1122378,\n",
            "        1122379, 1122380, 1122381, 1122382, 1122383, 1122384, 1122385, 1122386,\n",
            "        1122387, 1122388, 1122481, 1122482, 1122483, 1122484, 1122485, 1122486,\n",
            "        1122487, 1122488, 1122489, 1122490, 1122491, 1122492, 1122493, 1122494,\n",
            "        1122495, 1122496, 1122497, 1122498, 1122500, 1122502, 1122503, 1122504,\n",
            "        1122506, 1122507, 1122508, 1122509, 1122510, 1122511, 1122512, 1122513,\n",
            "        1122514, 1122609, 1122612, 1122616, 1122621, 1122622, 1138641, 1138642,\n",
            "        1138643, 1138644, 1138645, 1138646, 1138647, 1138648, 1138738, 1138740,\n",
            "        1138744, 1138768, 1138769, 1138770, 1138771, 1138772, 1138773, 1138774,\n",
            "        1138775, 1138776, 1138865, 1138866, 1138868, 1138872, 1138896, 1138897,\n",
            "        1138898, 1138899, 1138900, 1155028, 1155029, 1155030, 1155031, 1155032,\n",
            "        1155033, 1155034, 1155035, 1155036, 1155037, 1155038, 1155154, 1155156,\n",
            "        1155157, 1155158, 1155159, 1155160, 1155161, 1155162, 1155163, 1155164,\n",
            "        1155165, 1155166, 1155286, 1155288, 1155289, 1155290, 1171417, 1171418,\n",
            "        1171419, 1171420, 1171421, 1171422, 1171423, 1171424, 1171544, 1171545,\n",
            "        1171546, 1171547, 1171548, 1171549, 1171550, 1171551, 1171552, 1171674,\n",
            "        1171676, 1171678, 1171679, 1171680, 1187806, 1187807, 1187808, 1187809,\n",
            "        1187810, 1187811, 1187812, 1187932, 1187934, 1187935, 1187936, 1187937,\n",
            "        1187938, 1187939, 1187940, 1188063, 1188064, 1188065, 1188066, 1204192,\n",
            "        1204193, 1204194, 1204195, 1204196, 1204197, 1204198, 1204199, 1204320,\n",
            "        1204321, 1204322, 1204323, 1204324, 1204325, 1204326, 1204327, 1204448,\n",
            "        1204449, 1204450, 1204451, 1204452, 1204453, 1204454, 1204455, 1220578,\n",
            "        1220579, 1220580, 1220581, 1220582, 1220583, 1220584, 1220585, 1220706,\n",
            "        1220707, 1220708, 1220709, 1220710, 1220711, 1220712, 1220713, 1220834,\n",
            "        1220835, 1220836, 1220837, 1220838, 1220839, 1220840, 1220841, 1236968,\n",
            "        1236969, 1236970, 1236971, 1236972, 1237096, 1237097, 1237098, 1237099,\n",
            "        1237100, 1237101, 1237224, 1237225, 1237226, 1237227, 1253353, 1253354,\n",
            "        1253355, 1253356, 1253357, 1253358, 1253359, 1253360, 1253481, 1253482,\n",
            "        1253483, 1253484, 1253485, 1253486, 1253487, 1253488, 1253609, 1253610,\n",
            "        1253611, 1253612, 1253613, 1253614, 1253615, 1269739, 1269740, 1269741,\n",
            "        1269742, 1269743, 1269744, 1269745, 1269746, 1269747, 1269866, 1269867,\n",
            "        1269868, 1269869, 1269870, 1269871, 1269872, 1269873, 1269874, 1269875,\n",
            "        1269995, 1269996, 1269997, 1269998, 1269999, 1270000, 1286004, 1286126,\n",
            "        1286127, 1286128, 1286129, 1286130, 1286131, 1286132, 1286133, 1286134,\n",
            "        1286135, 1286254, 1286255, 1286256, 1286257, 1286258, 1286259, 1286260,\n",
            "        1286261, 1286262, 1286263, 1286384, 1286385, 1286386, 1286390, 1302388,\n",
            "        1302389, 1302390, 1302391, 1302392, 1302514, 1302515, 1302516, 1302517,\n",
            "        1302518, 1302519, 1302520, 1302521, 1302522, 1302523, 1302642, 1302643,\n",
            "        1302644, 1302645, 1302646, 1302647, 1302648, 1302649, 1302650, 1302651,\n",
            "        1302652, 1302774, 1302776, 1318778, 1318779, 1318780, 1318900, 1318901,\n",
            "        1318902, 1318903, 1318904, 1318905, 1318906, 1318907, 1318908, 1319028,\n",
            "        1319029, 1319030, 1319031, 1319032, 1319033, 1319034, 1319035, 1319036,\n",
            "        1319160, 1319164, 1335164, 1335290, 1335291, 1335292, 1335418, 1335419,\n",
            "        1335420, 1335548, 1351676, 1351804])\n",
            "indices shape: torch.Size([1224])\n",
            "indices: tensor([ 858284,  858285,  858286,  ..., 1071409, 1071410, 1071537])\n",
            "indices shape: torch.Size([1006])\n",
            "indices: tensor([ 829354,  829355,  829356,  ..., 1075504, 1091760, 1091888])\n",
            "indices shape: torch.Size([5481])\n",
            "indices: tensor([ 859445,  859446,  859447,  ..., 1218761, 1218762, 1218763])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indices shape: torch.Size([4858])\n",
            "indices: tensor([ 810803,  810931,  810932,  ..., 1255886, 1255887, 1255888])\n",
            "indices shape: torch.Size([3884])\n",
            "indices: tensor([ 843851,  843978,  843979,  ..., 1107909, 1107910, 1108037])\n",
            "indices shape: torch.Size([69403])\n",
            "indices: tensor([ 581179,  581307,  581435,  ..., 1499689, 1499816, 1499944])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-06dd0a630a79>\u001b[0m in \u001b[0;36m<cell line: 146>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-06dd0a630a79>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, num_epochs, learning_rate, max_dose)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mct_scans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mstructure_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-1a821fc6423e>\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mcomplete_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_paths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_patients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataBatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-1a821fc6423e>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, file_paths_to_load)\u001b[0m\n\u001b[1;32m    167\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mfile_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0mfile_data_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[0mfile_data_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_data_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure shape (128, 128, 128, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_data_dense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-1a821fc6423e>\u001b[0m in \u001b[0;36mdense_scatter\u001b[0;34m(self, sparse_vector)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdense_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_vector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"indices shape: {indices.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"indices: {indices}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    }
  ]
}